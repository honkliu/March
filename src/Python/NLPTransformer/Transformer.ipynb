{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder and Decoder implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        \n",
    "        self.eps = eps\n",
    "    def forward(self, x) :\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean)/(std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
    "None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask= None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "  \n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(d_k)\n",
    "    # scores = torch.matmul(query, key)/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    \n",
    "    print(p_attn.shape, value.shape)\n",
    "    a = torch.matmul(p_attn, value)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 64, 64]) torch.Size([5, 3, 64, 8])\n",
      "torch.Size([5, 3, 64, 8])\n",
      "torch.Size([5, 64, 3, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4.6194e-06,  4.5589e-02,  2.8807e-01, -2.0720e-01,  1.5754e-01,\n",
       "        -1.2248e-01,  8.9152e-02,  1.2591e-02,  4.1890e-02, -1.1686e-01,\n",
       "        -1.9751e-01, -2.2206e-01,  9.5626e-02,  6.8106e-02, -3.8782e-02,\n",
       "         3.0703e-01, -1.1599e-01,  2.3289e-01,  1.2031e-01,  1.5281e-02,\n",
       "        -8.2633e-02, -2.3833e-01,  5.5275e-03,  1.9202e-01],\n",
       "       grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = attention(q, k, v)\n",
    "k = torch.randn(5, 64, 24)\n",
    "q = torch.randn(5, 64, 24)\n",
    "v = torch.randn(5, 64, 24)\n",
    "\n",
    "lines = clones(nn.Linear(24, 24), 4)\n",
    "\n",
    "query, key, value = [l(x).view(5, -1, 3, 8).transpose(1,2) for l, x in zip(lines, (q, k, v))]\n",
    "x, atten = attention(query, key, value)\n",
    "print(x.shape)\n",
    "a = x.transpose(1,2)\n",
    "print(a.shape)\n",
    "lines[-1](a.contiguous().view(5, -1, 24)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        # Actually, the NN.Linear is mapping from d_model to h * d_k\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        relu_res = F.relu(self.w_1(x))\n",
    "        print(relu_res)\n",
    "        return self.w_2(self.dropout(relu_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 64, 64]) torch.Size([5, 3, 64, 8])\n",
      "torch.Size([5, 64, 24])\n",
      "tensor([-0.1161, -0.1371, -0.0631,  0.0147, -0.0827, -0.0352, -0.0950, -0.2493,\n",
      "        -0.2655,  0.0924, -0.0802, -0.2124, -0.0972,  0.0154,  0.1902,  0.1486,\n",
      "        -0.1724, -0.2983, -0.1251,  0.1275,  0.0442,  0.0799,  0.0587, -0.1522],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.1704, 0.0985, 0.1897, 0.2031, 0.2169, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0307, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1984, 0.2026,\n",
      "        0.1426, 0.0000, 0.0000, 0.0000, 0.1596, 0.1340, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.1339, 0.0000, 0.0739, 0.0000, 0.1184, 0.1656, 0.1383,\n",
      "        0.0000, 0.0000, 0.1392, 0.0000, 0.0000, 0.0000, 0.0000, 0.1339, 0.0928,\n",
      "        0.0000, 0.0000, 0.0000, 0.0696, 0.0000, 0.0000, 0.0526, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.2409, 0.0627, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<ReluBackward>)\n",
      "tensor([ 0.0159,  0.0456,  0.0589, -0.0717,  0.0624,  0.0040,  0.0549, -0.0995,\n",
      "         0.0187, -0.0170,  0.0618,  0.0861, -0.0469, -0.0118, -0.0879, -0.0173,\n",
      "         0.0310,  0.1086,  0.1142, -0.0357,  0.0864, -0.1086, -0.0333, -0.0160],\n",
      "       grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(5, 64, 24)\n",
    "q = torch.randn(5, 64, 24)\n",
    "v = torch.randn(5, 64, 24)\n",
    "\n",
    "a = MultiHeadedAttention(3, 24)\n",
    "res = a(q, k, v)\n",
    "print(res.shape)\n",
    "print(res[0][0])\n",
    "ff = PositionwiseFeedForward(24, 64)\n",
    "print(ff(res[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Embeddings(24, 10)\n",
    "# a('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shuguang', 'Liu']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x1a37e3316a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD='<s>'\n",
    "EOS_WORD='</s>'\n",
    "BLANK_WORD=\"<blank>\"\n",
    "\n",
    "print(tokenize_de(\"Shuguang Liu\"))\n",
    "\n",
    "SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
    "TGT = data.Field(tokenize=tokenize_en, init_token=BOS_WORD, eos_token=EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "MAX_LEN= 100\n",
    "\n",
    "train, val, test = datasets.IWSLT.splits(\n",
    "    exts = ('.de', '.en'), \n",
    "    fields = (SRC, TGT), \n",
    "    filter_pred = lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)\n",
    "\n",
    "MIN_FREQ=2\n",
    "SRC.build_vocab(train.src, min_freq = MIN_FREQ)\n",
    "TGT.build_vocab(train.trg, min_freq = MIN_FREQ)\n",
    "\n",
    "train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataiter, model):\n",
    "    for i, batch in enumerate(dataiter):\n",
    "        print(i)\n",
    "        dest = batch.trg.transpose(0,1)\n",
    "        ovec = model(dest)\n",
    "        print(\"src-->\", dest)\n",
    "        print(\"to-->\", ovec)\n",
    "        if (i>10):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "#define the transformer\n",
    "#model = make_model(len(SRC.vocab), len(RGT.vocab), N=6) \n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "BATCHSIZE=5\n",
    "\n",
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "train_iter = Iterator(train, batch_size=BATCHSIZE, device=-1, sort=False, sort_within_batch=False,repeat=False)\n",
    "val_iter = Iterator(val, batch_size=BATCHSIZE, device=-1, sort=False, sort_within_batch=False,repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "epocstart\n",
      "0\n",
      "src--> tensor([[    2,   362,    65,    16,    14,   778,     5,     3,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1],\n",
      "        [    2, 20782,     4,    12,    73,     7,   177,   216,    11,     6,\n",
      "           316,    22,    24,  1762,   133,     9,     7,    86,    64,     5,\n",
      "             3],\n",
      "        [    2,    32,     4,   435,    15,    43,   232,    40,   116,    47,\n",
      "            10,  3710,  4070,    10,  1776,   300,     5,     3,     1,     1,\n",
      "             1],\n",
      "        [    2,    19,     6,     0,   200,    68,  1129,    11,    26,    35,\n",
      "            39,  1565,     5,     3,     1,     1,     1,     1,     1,     1,\n",
      "             1],\n",
      "        [    2,    58,   140,     7,   365,    46,   675,   877,    11,   106,\n",
      "             5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [-3.0596, -2.3139],\n",
      "         [-1.7119,  0.9648],\n",
      "         [-0.0472,  0.3280],\n",
      "         [-0.8812, -0.3272],\n",
      "         [ 2.8792, -1.5212],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.0719, -0.2527],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [ 1.6239,  0.1034],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.4818, -0.7250],\n",
      "         [-0.3129,  0.7211],\n",
      "         [-0.8815, -0.5337],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-2.1193,  0.3465],\n",
      "         [-0.5039,  0.3571],\n",
      "         [ 1.6521, -0.1554],\n",
      "         [ 2.4263, -2.5624],\n",
      "         [ 0.1603,  1.2059],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 0.8440,  0.5785],\n",
      "         [ 1.2536,  0.3521],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-2.0469,  2.2113],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.0177,  4.9310],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-2.3291,  0.0659],\n",
      "         [ 0.4623,  1.4062],\n",
      "         [-1.6291, -2.9115],\n",
      "         [ 0.8351, -1.1424],\n",
      "         [-0.6483, -0.1713],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-0.6762,  1.7224],\n",
      "         [-2.5232, -0.4124],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 0.2794, -1.1634],\n",
      "         [ 3.1986,  2.1770],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-0.6830, -0.5819],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-1.3445,  4.7891],\n",
      "         [-0.7462,  1.0868],\n",
      "         [-1.3266, -0.8929],\n",
      "         [ 2.0647, -1.0191],\n",
      "         [-0.8815, -0.5337],\n",
      "         [-0.6126,  0.6062],\n",
      "         [-0.2176,  0.7039],\n",
      "         [-1.0722, -1.3983],\n",
      "         [-1.9358, -0.8649],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 2.7113,  1.0924],\n",
      "         [-0.2971, -1.1510],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-1.2474, -1.4669],\n",
      "         [ 1.3756,  0.7719],\n",
      "         [-0.0283, -0.5361],\n",
      "         [ 0.2073, -0.0860],\n",
      "         [-0.8815, -0.5337],\n",
      "         [ 0.2341, -0.0386],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]]], grad_fn=<MulBackward>)\n",
      "1\n",
      "src--> tensor([[    2,   128,    17,    47,  5232,    85,   775,    12,   494,     7,\n",
      "             6,   938,     8,   300,     5,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   234,    15,    77,     6,  5174,     4,   131,   527,   527,\n",
      "           849,     7,  1277,     5,     3,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   234,    26,    25,    84,   277,     4,  4374,     4,    26,\n",
      "           172,   205,   129,   130,    40, 15478,     9,   131,     4,    21,\n",
      "          1449,     4,    15,   549,    16,    10,   151,  1278,     5,    88,\n",
      "           107,     6,   304,    15,   101,    30,     6,   655,    11,    15,\n",
      "            73,     5,    21,     3],\n",
      "        [    2,   111,   652,   331,  2217,     4,    51,    26,   190,  2253,\n",
      "            13,    84,  3600,  1202,     5,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   166,    15,    23,    10, 15580,  2121,     9,    15,    23,\n",
      "           262,   159,  1772,     7,  2594,  7004,     4,    15,    43,    67,\n",
      "             7,   129,    39,    51,  4222,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [ 2.4067, -1.6465],\n",
      "         [ 0.1173,  1.4184],\n",
      "         [-0.6483, -0.1713],\n",
      "         [-1.0464, -1.5367],\n",
      "         [-0.9020,  0.4851],\n",
      "         [-2.1667,  3.1404],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [ 0.5180, -1.1472],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 1.6821, -0.4457],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 3.1986,  2.1770],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.0152,  0.9429],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-0.3291, -2.0185],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-1.3477, -1.2060],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-1.6355,  0.5754],\n",
      "         [-0.2871, -0.0752],\n",
      "         [-0.2871, -0.0752],\n",
      "         [ 2.0028, -0.1591],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 0.3833,  3.5622],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.0152,  0.9429],\n",
      "         [-0.6126,  0.6062],\n",
      "         [-0.1171, -0.3913],\n",
      "         [-2.0918, -3.6161],\n",
      "         [ 1.6087,  0.9573],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.6610,  1.9383],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.6126,  0.6062],\n",
      "         [-2.8628, -0.5724],\n",
      "         [-0.7158, -1.6037],\n",
      "         [ 2.1745, -0.6827],\n",
      "         [-1.5697,  0.2079],\n",
      "         [-1.6291, -2.9115],\n",
      "         [ 1.3436, -0.0290],\n",
      "         [-0.5384,  0.7528],\n",
      "         [-1.6355,  0.5754],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 3.1944, -0.2277],\n",
      "         [-1.9916,  0.1066],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [ 1.3260,  1.9189],\n",
      "         [-0.0472,  0.3280],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-0.2249, -0.5967],\n",
      "         [-0.1817,  1.8344],\n",
      "         [-0.1925,  3.2472],\n",
      "         [ 1.7284,  0.6714],\n",
      "         [-3.4545,  0.1708],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-1.2669, -1.0062],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [ 1.5603,  0.0474],\n",
      "         [-2.6698,  0.3895],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-0.3101,  0.7079],\n",
      "         [-0.8815, -0.5337],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [ 1.6239,  0.1034],\n",
      "         [-0.1925,  3.2472],\n",
      "         [ 3.1944, -0.2277],\n",
      "         [-0.9256,  0.5580]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 1.7539,  2.3357],\n",
      "         [ 0.0606,  1.3005],\n",
      "         [-0.1327, -0.1665],\n",
      "         [ 1.8888,  0.2105],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-1.8924,  1.3922],\n",
      "         [-0.6126,  0.6062],\n",
      "         [-0.8931,  1.6385],\n",
      "         [-1.8942,  1.0433],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [-2.0918, -3.6161],\n",
      "         [-0.0875,  1.2123],\n",
      "         [ 0.7457, -2.0165],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.4179,  0.8692],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-0.4520, -0.9377],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 0.0476, -1.7139],\n",
      "         [ 0.8825,  0.9458],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-0.4520, -0.9377],\n",
      "         [-2.2845, -0.2296],\n",
      "         [ 0.6511,  1.7621],\n",
      "         [-0.2084, -1.0047],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.4629,  0.7185],\n",
      "         [-0.0532, -0.5924],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-2.3291,  0.0659],\n",
      "         [-1.9422, -0.6282],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 2.1745, -0.6827],\n",
      "         [-1.0722, -1.3983],\n",
      "         [-1.8924,  1.3922],\n",
      "         [-0.7277,  1.3716],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [ 0.1719, -2.3358]]], grad_fn=<MulBackward>)\n",
      "2\n",
      "src--> tensor([[    2,   136,    11,   275,    14,    11,    15,    25,     7,    25,\n",
      "            10,   285,  1174,    56,   648,    24,   587,     5,     3,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    2,    12,   227,    41,  4448,    41,   137,   643,    22,    13,\n",
      "            10, 10686,  2094,     4,    16,    22,    51,  4094,    30,   137,\n",
      "           135,     4,    15,    66,    31,     3],\n",
      "        [    2,    19,  2082,     7,     6,  2703,   724,     4,    40,    11,\n",
      "          3993,     4,    10,  2483,  1285,   281,   226,    13,     6,   464,\n",
      "             5,     3,     1,     1,     1,     1],\n",
      "        [    2,   676,   548,    28,    83,   339,    13,    82,  3688,  1818,\n",
      "           149,    15,     5,     3,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    2,    19,   280,    18,   261,    13,    52,  5873,   464,     5,\n",
      "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [ 0.2334,  0.7556],\n",
      "         [-0.8815, -0.5337],\n",
      "         [ 1.6772,  0.2359],\n",
      "         [-0.8812, -0.3272],\n",
      "         [-0.8815, -0.5337],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-0.1171, -0.3913],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.1171, -0.3913],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 2.0377,  1.9221],\n",
      "         [ 1.1268,  2.5322],\n",
      "         [-0.7530, -0.8337],\n",
      "         [ 1.5164,  0.6412],\n",
      "         [ 1.6521, -0.1554],\n",
      "         [ 0.9857,  0.2600],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-1.0948, -0.8486],\n",
      "         [ 1.3796, -0.5014],\n",
      "         [-0.8712,  0.6857],\n",
      "         [ 1.3796, -0.5014],\n",
      "         [-1.3978,  1.0991],\n",
      "         [ 1.7484, -0.7688],\n",
      "         [-0.5039,  0.3571],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 0.0485,  0.1368],\n",
      "         [-1.1651, -1.2135],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.0472,  0.3280],\n",
      "         [-0.5039,  0.3571],\n",
      "         [-1.8924,  1.3922],\n",
      "         [ 1.8917, -0.4098],\n",
      "         [-2.6698,  0.3895],\n",
      "         [-1.3978,  1.0991],\n",
      "         [ 0.6376,  2.3884],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [ 0.8795, -1.6551],\n",
      "         [ 0.8815,  0.3188],\n",
      "         [-0.9256,  0.5580]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-0.6830, -0.5819],\n",
      "         [ 0.3153, -1.9447],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.9935, -1.8053],\n",
      "         [ 1.4068, -0.6922],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-1.6291, -2.9115],\n",
      "         [-0.8815, -0.5337],\n",
      "         [-2.7533, -0.3488],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-0.2048,  1.7558],\n",
      "         [-0.9663,  2.8975],\n",
      "         [ 0.9848, -0.0403],\n",
      "         [-2.6243,  0.0130],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-1.7285,  0.2254],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.6608,  2.7488],\n",
      "         [-0.0804,  1.4882],\n",
      "         [-3.0662,  0.0001],\n",
      "         [ 0.7803, -0.5623],\n",
      "         [ 1.7451, -0.5573],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [-2.7675, -1.0022],\n",
      "         [ 0.2464, -0.4518],\n",
      "         [-0.9744,  1.2321],\n",
      "         [-3.1037,  0.6578],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-0.6830, -0.5819],\n",
      "         [ 0.6371, -0.1677],\n",
      "         [-1.3014,  2.2870],\n",
      "         [ 2.7161, -1.0527],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [-0.9997,  1.6954],\n",
      "         [-0.4179,  1.9581],\n",
      "         [-1.7285,  0.2254],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]]], grad_fn=<MulBackward>)\n",
      "3\n",
      "src--> tensor([[    2,   247,    29,    18,    86,   168,    98,     6,  2412,    31,\n",
      "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,   136,    15,    43,   164,    14,    15,    81,  6640,    82,\n",
      "          1913,     9,  1637,     6,  1002,     4,   785,    69,   224,     4,\n",
      "          6640,    82,  1913,     9,  1637,     6,  1002,     5,     3],\n",
      "        [    2,   689,   167,    16,    13,  1098,     4,  2863,    22,     6,\n",
      "          2283,  1136,     5,     3,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2, 14295,    54,    88,    87, 13854,    42,     8,    60,    48,\n",
      "            13,    10,   586,   100,     4,   205,   120,    15,    66,     5,\n",
      "             3,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,    92,    14,    10,   251,  1078,     8,  2989,    11,   936,\n",
      "             7,  1529,    15,  2562,     5,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [ 3.1143,  1.4401],\n",
      "         [-0.4206, -1.4973],\n",
      "         [-1.3014,  2.2870],\n",
      "         [ 0.8440,  0.5785],\n",
      "         [-2.2878,  0.9279],\n",
      "         [ 1.2726, -1.3975],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-0.3695,  0.3560],\n",
      "         [ 0.8815,  0.3188],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.2334,  0.7556],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-2.3291,  0.0659],\n",
      "         [ 0.4634, -0.4847],\n",
      "         [-0.8812, -0.3272],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-2.4933,  0.2415],\n",
      "         [ 1.0013,  0.3614],\n",
      "         [-2.7675, -1.0022],\n",
      "         [ 0.9901, -0.7554],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 0.4576, -1.8335],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.9160,  0.7121],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-1.1256, -0.2355],\n",
      "         [-3.0459, -0.8058],\n",
      "         [ 0.4192,  1.8965],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.0013,  0.3614],\n",
      "         [-2.7675, -1.0022],\n",
      "         [ 0.9901, -0.7554],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 0.4576, -1.8335],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.9160,  0.7121],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 2.3200, -0.8083],\n",
      "         [-0.9468, -0.2023],\n",
      "         [-0.0472,  0.3280],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [-0.2073, -1.3903],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-2.0745, -0.4135],\n",
      "         [-0.5039,  0.3571],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-0.3443,  1.2683],\n",
      "         [-0.8583, -0.8289],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-1.5534, -1.0602],\n",
      "         [-1.0169, -0.3010],\n",
      "         [ 1.7284,  0.6714],\n",
      "         [ 3.1427, -0.5949],\n",
      "         [ 1.1861, -0.6464],\n",
      "         [ 0.3684,  1.0023],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-0.4326, -1.2946],\n",
      "         [-0.9138,  1.3759],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 3.4919,  3.6099],\n",
      "         [ 0.6310, -1.5822],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.7158, -1.6037],\n",
      "         [ 1.8614,  1.5225],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [ 0.8795, -1.6551],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.8445, -0.4700],\n",
      "         [-0.8812, -0.3272],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-0.1695,  3.2665],\n",
      "         [ 0.2641,  1.0885],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 2.9674,  2.3892],\n",
      "         [-0.8815, -0.5337],\n",
      "         [-1.1779, -1.3546],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.6285,  1.6605],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-0.3034, -0.3067],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [ 0.1719, -2.3358]]], grad_fn=<MulBackward>)\n",
      "4\n",
      "src--> tensor([[    2,    88,    35,    77,     6,   728,  1539,     4,     9,    12,\n",
      "            29,    34,    66,    63,    15,    35,    77,     6,  2735,  2920,\n",
      "            44,     5,     3,     1,     1,     1,     1,     1],\n",
      "        [    2,    19,    63,    18,    55,   142,    46,   567,    17,   866,\n",
      "             8,    20,  1188,     9, 36063,    16,     4,    16,    17,    10,\n",
      "          2797,   222,     4,   118, 11122,  3495,     5,     3],\n",
      "        [    2,    32,    18,   474,    39,     7,  1625,     6,   396,    64,\n",
      "             9,     7, 27471,     6,   823,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,    59,    16,    22,    81,     4,     7,    95,     4,    61,\n",
      "           199,    41,    10,  1708,     5,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,    19,    16,   101,   399,     6,   762,    24,   422,   902,\n",
      "             5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [ 1.7284,  0.6714],\n",
      "         [-0.2176,  0.7039],\n",
      "         [-0.3291, -2.0185],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-0.9059,  1.4165],\n",
      "         [ 0.7253,  0.5210],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-0.4206, -1.4973],\n",
      "         [ 0.1066, -1.3214],\n",
      "         [ 0.8795, -1.6551],\n",
      "         [-0.3635,  2.9131],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-0.2176,  0.7039],\n",
      "         [-0.3291, -2.0185],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-0.6053, -2.7616],\n",
      "         [ 1.7082, -0.3212],\n",
      "         [ 2.8323, -0.9417],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-0.6830, -0.5819],\n",
      "         [-0.3635,  2.9131],\n",
      "         [-1.3014,  2.2870],\n",
      "         [-0.4065,  0.5112],\n",
      "         [ 1.0280,  0.2751],\n",
      "         [ 1.3756,  0.7719],\n",
      "         [-1.5981, -0.8198],\n",
      "         [ 0.1173,  1.4184],\n",
      "         [ 1.4380,  2.3670],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-0.3502, -2.2233],\n",
      "         [-0.5984, -0.6477],\n",
      "         [-0.5384,  0.7528],\n",
      "         [-2.6329, -1.3983],\n",
      "         [-0.0472,  0.3280],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.0472,  0.3280],\n",
      "         [ 0.1173,  1.4184],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-2.2158,  0.7566],\n",
      "         [-0.3753,  0.0820],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-1.1141, -1.5008],\n",
      "         [-0.0502, -0.8892],\n",
      "         [-0.4309, -1.3352],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-2.0469,  2.2113],\n",
      "         [-1.3014,  2.2870],\n",
      "         [-0.5490,  1.7904],\n",
      "         [-1.0722, -1.3983],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-2.1067,  1.5989],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 2.5320, -0.2439],\n",
      "         [ 1.2536,  0.3521],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 1.6944, -0.1766],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.6201, -0.9208],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.2486, -0.1950],\n",
      "         [-0.0472,  0.3280],\n",
      "         [-0.5039,  0.3571],\n",
      "         [-2.4933,  0.2415],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.9758, -2.8232],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.8584, -1.1429],\n",
      "         [ 0.1825,  1.8125],\n",
      "         [ 1.3796, -0.5014],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 1.2771, -1.1735],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-0.6830, -0.5819],\n",
      "         [-0.0472,  0.3280],\n",
      "         [ 1.5603,  0.0474],\n",
      "         [ 0.3712, -1.8415],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-0.6809,  1.8783],\n",
      "         [ 1.6521, -0.1554],\n",
      "         [-0.5468,  0.7313],\n",
      "         [ 0.2886, -0.0575],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]]], grad_fn=<MulBackward>)\n",
      "5\n",
      "src--> tensor([[    2,    92,    14,   151,   562,     4,    72,    16,   275,    11,\n",
      "          3084,   540,  2734, 32902,    14,    10, 17538,  2599,     5,     3,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,    62,    74,   395,    15,   798,     4,    15,    43,   439,\n",
      "             7,   160,    69,    30,    99,   151,   523,     4,   242,     5,\n",
      "             3,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,    58,    23,  2717,    30,    20,   100,     8,   278,     5,\n",
      "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,    59,    44,    17,    10,  2677,    24,    11,   242,     5,\n",
      "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,    62,  2278,     4,   619,    26,    29,     4,  2220,    26,\n",
      "           255,    95,   221,     4,  3477,    95,     7,   129,    40,    57,\n",
      "          2292,   181,   159,    30,  6578,     9,  3043,     5,     3]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [ 0.8445, -0.4700],\n",
      "         [-0.8812, -0.3272],\n",
      "         [-0.2249, -0.5967],\n",
      "         [ 1.3939,  0.0345],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.5963,  1.6625],\n",
      "         [-0.0472,  0.3280],\n",
      "         [ 1.6772,  0.2359],\n",
      "         [-0.8815, -0.5337],\n",
      "         [-2.0902, -1.5820],\n",
      "         [ 0.3125,  0.5991],\n",
      "         [ 0.5228, -0.0330],\n",
      "         [-0.9137, -0.4974],\n",
      "         [-0.8812, -0.3272],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 1.0319,  1.8699],\n",
      "         [ 0.8326,  2.1255],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 1.9408, -0.5693],\n",
      "         [ 0.2765,  3.3031],\n",
      "         [-0.4747, -0.2841],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-1.0297,  0.2192],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-2.3291,  0.0659],\n",
      "         [ 0.8109,  0.0997],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 0.1532, -1.4249],\n",
      "         [-3.0459, -0.8058],\n",
      "         [-2.6698,  0.3895],\n",
      "         [-0.2718,  1.4740],\n",
      "         [-0.2249, -0.5967],\n",
      "         [ 0.4443,  1.4862],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.7296, -0.3088],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 2.7113,  1.0924],\n",
      "         [-0.4520, -0.9377],\n",
      "         [-0.4281,  1.5296],\n",
      "         [-2.6698,  0.3895],\n",
      "         [-0.3502, -2.2233],\n",
      "         [ 0.6310, -1.5822],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 0.0673,  1.5391],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.2486, -0.1950],\n",
      "         [ 2.8323, -0.9417],\n",
      "         [ 0.1173,  1.4184],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 1.1333, -1.5362],\n",
      "         [ 1.6521, -0.1554],\n",
      "         [-0.8815, -0.5337],\n",
      "         [ 1.7296, -0.3088],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 1.9408, -0.5693],\n",
      "         [ 0.0308, -0.9856],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.3060, -0.0270],\n",
      "         [-0.6126,  0.6062],\n",
      "         [-0.4206, -1.4973],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.8506,  0.9878],\n",
      "         [-0.6126,  0.6062],\n",
      "         [-1.2157, -1.0846],\n",
      "         [-0.9758, -2.8232],\n",
      "         [-0.6504,  0.9301],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-1.3120,  2.5911],\n",
      "         [-0.9758, -2.8232],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 2.1745, -0.6827],\n",
      "         [-1.6291, -2.9115],\n",
      "         [-0.8009,  0.0840],\n",
      "         [ 0.1893, -0.4200],\n",
      "         [-1.7833,  0.0634],\n",
      "         [ 0.6511,  1.7621],\n",
      "         [-2.6698,  0.3895],\n",
      "         [ 0.2855,  1.1064],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 1.1013, -0.5104],\n",
      "         [-0.1925,  3.2472],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [-0.9256,  0.5580]]], grad_fn=<MulBackward>)\n",
      "6\n",
      "src--> tensor([[    2,    32,    16,    17,    39,    36,    49,   212,     5,    45,\n",
      "            17,    39,    36,    82,   212,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    2,   141,   179,     4,    12,   176,    10,   358,    70,   921,\n",
      "             5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    2,    32,   165,     8,   124,  2776,    27,  3536,     4, 21225,\n",
      "             9, 14482,    27,  5828,  2078,  5487,     4,     9,     6,  2012,\n",
      "          5064,    84,  2669,     9,  5886,    11,    70,     6,    91,    26,\n",
      "           126,   130,   306,     4,     6,   574,    80,    38,  7262,    69,\n",
      "            50,    84,  4925,     5,     3],\n",
      "        [    2,   206,    15,    61,   119,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    2,    62, 33530,    14,   302,   586,   379,    42,   340,   491,\n",
      "             5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [-2.0469,  2.2113],\n",
      "         [-0.0472,  0.3280],\n",
      "         [ 0.1173,  1.4184],\n",
      "         [-1.0722, -1.3983],\n",
      "         [ 0.3116, -3.6889],\n",
      "         [-1.2236, -0.7050],\n",
      "         [ 1.1470, -0.4558],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.5510,  0.7353],\n",
      "         [ 0.1173,  1.4184],\n",
      "         [-1.0722, -1.3983],\n",
      "         [ 0.3116, -3.6889],\n",
      "         [-2.7675, -1.0022],\n",
      "         [ 1.1470, -0.4558],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-0.2526,  1.8956],\n",
      "         [ 0.9425,  0.4281],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-1.0306, -0.2504],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 2.7125,  2.1857],\n",
      "         [ 0.3373, -0.0375],\n",
      "         [-0.4358,  0.0064],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-2.0469,  2.2113],\n",
      "         [-2.2691,  1.9849],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-1.9381, -0.3054],\n",
      "         [ 0.9355, -1.8995],\n",
      "         [-1.0895,  0.8959],\n",
      "         [-1.6663,  0.9497],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.2229, -0.6522],\n",
      "         [-0.5384,  0.7528],\n",
      "         [-2.2645,  1.2001],\n",
      "         [-1.0895,  0.8959],\n",
      "         [-0.0090, -0.1407],\n",
      "         [ 2.1542, -1.3059],\n",
      "         [-1.7338,  1.9475],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-1.1419,  0.9535],\n",
      "         [ 0.9590,  2.4688],\n",
      "         [-2.0918, -3.6161],\n",
      "         [ 0.8734,  2.3663],\n",
      "         [-0.5384,  0.7528],\n",
      "         [-2.0725,  0.2861],\n",
      "         [-0.8815, -0.5337],\n",
      "         [ 0.3373, -0.0375],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-0.2911,  1.5609],\n",
      "         [-0.6126,  0.6062],\n",
      "         [ 0.9944,  1.4640],\n",
      "         [-1.5697,  0.2079],\n",
      "         [ 0.0282, -0.0906],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.6067, -0.4727],\n",
      "         [ 1.7834, -2.0851],\n",
      "         [-0.9192, -0.1235],\n",
      "         [ 0.9224,  0.7257],\n",
      "         [-3.0459, -0.8058],\n",
      "         [-0.5841,  0.8145],\n",
      "         [-2.0918, -3.6161],\n",
      "         [-1.5791,  0.2921],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.9932, -2.1217],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [ 0.8584, -1.1429],\n",
      "         [ 0.1483, -1.1064],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 1.9408, -0.5693],\n",
      "         [ 2.9287,  1.2156],\n",
      "         [-0.8812, -0.3272],\n",
      "         [-0.7573,  2.3041],\n",
      "         [ 3.4919,  3.6099],\n",
      "         [ 1.7119,  0.8583],\n",
      "         [ 0.3684,  1.0023],\n",
      "         [-1.3196, -1.1220],\n",
      "         [ 1.1685,  0.6244],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [ 0.1719, -2.3358]]], grad_fn=<MulBackward>)\n",
      "7\n",
      "src--> tensor([[    2, 11980,    54,   645,    20,   245,     4,    12,   153,   142,\n",
      "            10,   225,  1552,     5,     3,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    2,    12,    90,    67,    64,     7,   129,    64,    24,    95,\n",
      "             4,     6,   312,     8,    57,   274,     5,    21,     3,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    2,    32,    16,   919,    47,    10,   182,   212,     5,     3,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [    2,    32,    12,   110,     7,   200,    10,   123,   220,     8,\n",
      "            10,   152,  1098,     8,    85,    18,    35,   592,   129,    40,\n",
      "            60,   102,    13,    10,   123,   220,     8,    10,   152,   100,\n",
      "             5,     3],\n",
      "        [    2,   128,    17,    10,   331,   309,  1173,    13,   488,     5,\n",
      "            88,    87,  8739,    52,   570,   181,   450,    98,    10,   880,\n",
      "           681,     5,     3,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [ 1.7856, -4.2679],\n",
      "         [-1.0169, -0.3010],\n",
      "         [ 0.5005, -0.6034],\n",
      "         [-0.3502, -2.2233],\n",
      "         [ 0.4077,  0.8059],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [ 0.3957,  2.1238],\n",
      "         [ 1.0280,  0.2751],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-1.0514,  0.1709],\n",
      "         [ 1.7962,  1.6758],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [ 0.0915, -0.9003],\n",
      "         [-1.9422, -0.6282],\n",
      "         [ 1.2536,  0.3521],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 2.1745, -0.6827],\n",
      "         [ 1.2536,  0.3521],\n",
      "         [ 1.6521, -0.1554],\n",
      "         [-0.9758, -2.8232],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.6090,  1.2390],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-0.8009,  0.0840],\n",
      "         [-1.1631, -0.0379],\n",
      "         [-0.1925,  3.2472],\n",
      "         [ 3.1944, -0.2277],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-2.0469,  2.2113],\n",
      "         [-0.0472,  0.3280],\n",
      "         [-0.4997,  0.2452],\n",
      "         [-0.6483, -0.1713],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 0.1188, -1.5172],\n",
      "         [ 1.1470, -0.4558],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-2.0469,  2.2113],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-0.8921, -0.1969],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.7462,  1.0868],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 2.0964,  0.7701],\n",
      "         [-2.0975,  0.5494],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-0.3442,  0.0864],\n",
      "         [-0.2073, -1.3903],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-0.9020,  0.4851],\n",
      "         [-1.3014,  2.2870],\n",
      "         [-0.2176,  0.7039],\n",
      "         [ 0.3942, -1.3037],\n",
      "         [ 2.1745, -0.6827],\n",
      "         [-1.6291, -2.9115],\n",
      "         [-0.4326, -1.2946],\n",
      "         [-1.3645,  2.3630],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 2.0964,  0.7701],\n",
      "         [-2.0975,  0.5494],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-0.3442,  0.0864],\n",
      "         [ 0.6310, -1.5822],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 2.4067, -1.6465],\n",
      "         [ 0.1173,  1.4184],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-0.1327, -0.1665],\n",
      "         [ 2.4010, -0.8075],\n",
      "         [-0.7538, -2.6189],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [ 0.6105, -0.0537],\n",
      "         [-0.1925,  3.2472],\n",
      "         [ 1.7284,  0.6714],\n",
      "         [ 3.1427, -0.5949],\n",
      "         [-1.8763,  0.4586],\n",
      "         [-0.9997,  1.6954],\n",
      "         [-3.3076,  3.1282],\n",
      "         [-1.7833,  0.0634],\n",
      "         [-1.8030, -0.5484],\n",
      "         [ 1.2726, -1.3975],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 1.4230,  2.2469],\n",
      "         [ 0.3842, -1.8455],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]]], grad_fn=<MulBackward>)\n",
      "8\n",
      "src--> tensor([[    2,    32,   280,    15,    43,   363,     6,   157,  9753,    41,\n",
      "             6,  2610,    11,    78,    97,  2755,    13,    20,     4,    79,\n",
      "            14,    71,  2915,    30,    60,  2752,     4,    48,    29,    34,\n",
      "          1094,     7,   321,    16,  1034,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,    12,  1006,    69,    28,    10,   292,  2119,    13, 10861,\n",
      "             5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,    45,  1062,   116,    11,    80,  1241,    95,    42,   221,\n",
      "             5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,  1821,  4519,  1772,     6,  5225,     8,  2410,  3422,    27,\n",
      "          3588,  3642,    27,     7, 20249,     6,  9904,     8,  1029,   491,\n",
      "             7,   414,    46,  1539,    17, 11484,     8,  5225,     5,     3,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    2,   105,     4,  6047, 10557,    14,    52,  4213, 33675,    27,\n",
      "            16,  9372,    13,    42,     6,  5984,    27,    53,    16,    14,\n",
      "           331,   285,  6118,   308,     4,     9,     4, 13309,    37,    13,\n",
      "         11505,    56,   683,  7979,    14,    10,   119,  1207,   308,  2045,\n",
      "             7,    11,     5,     3]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [-2.0469,  2.2113],\n",
      "         [ 0.6371, -0.1677],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-2.3291,  0.0659],\n",
      "         [ 1.4339, -1.0394],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-1.8864, -1.1271],\n",
      "         [ 0.4614, -2.3666],\n",
      "         [ 1.3796, -0.5014],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.7865,  1.1392],\n",
      "         [-0.8815, -0.5337],\n",
      "         [-0.5215, -1.0630],\n",
      "         [ 0.3117,  1.4283],\n",
      "         [-2.3581, -1.5884],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [-0.3502, -2.2233],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.1073,  0.4892],\n",
      "         [-0.8812, -0.3272],\n",
      "         [ 1.0681,  2.2660],\n",
      "         [ 2.5427, -0.1481],\n",
      "         [-2.6698,  0.3895],\n",
      "         [-0.4326, -1.2946],\n",
      "         [-0.3064, -1.1517],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.9138,  1.3759],\n",
      "         [-0.4206, -1.4973],\n",
      "         [ 0.1066, -1.3214],\n",
      "         [ 1.9411,  1.2754],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.0601, -1.4669],\n",
      "         [-0.0472,  0.3280],\n",
      "         [-0.9932, -0.2622],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-0.7885, -2.6604],\n",
      "         [-3.0459, -0.8058],\n",
      "         [-3.0662,  0.0001],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-0.8899,  0.8340],\n",
      "         [-0.9641, -1.2621],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [-0.3413, -0.6461],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-0.5510,  0.7353],\n",
      "         [ 1.5412, -0.5101],\n",
      "         [ 0.8351, -1.1424],\n",
      "         [-0.8815, -0.5337],\n",
      "         [ 1.7834, -2.0851],\n",
      "         [ 0.8805, -0.5688],\n",
      "         [-0.9758, -2.8232],\n",
      "         [ 0.3684,  1.0023],\n",
      "         [-0.6504,  0.9301],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 1.3552,  0.8798],\n",
      "         [ 1.2967, -1.0816],\n",
      "         [-0.2084, -1.0047],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-1.3521, -1.4269],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-1.5054, -1.1348],\n",
      "         [-0.8571, -0.8718],\n",
      "         [-1.0895,  0.8959],\n",
      "         [ 0.3937,  0.0728],\n",
      "         [-1.9222,  0.5819],\n",
      "         [-1.0895,  0.8959],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 3.0853,  1.8426],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.3333, -1.4600],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-0.7806, -0.0293],\n",
      "         [ 1.1685,  0.6244],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.0253, -0.5517],\n",
      "         [ 1.3756,  0.7719],\n",
      "         [ 0.7253,  0.5210],\n",
      "         [ 0.1173,  1.4184],\n",
      "         [ 2.2193,  0.3479],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-1.3521, -1.4269],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 1.1094,  2.0835],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.8098,  0.0485],\n",
      "         [ 0.2589, -1.6464],\n",
      "         [-0.8812, -0.3272],\n",
      "         [-0.9997,  1.6954],\n",
      "         [ 0.2960,  0.0122],\n",
      "         [-0.3355,  1.0228],\n",
      "         [-1.0895,  0.8959],\n",
      "         [-0.0472,  0.3280],\n",
      "         [ 0.4630,  1.9922],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [ 0.3684,  1.0023],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-0.1460,  1.1465],\n",
      "         [-1.0895,  0.8959],\n",
      "         [-0.4972,  3.2895],\n",
      "         [-0.0472,  0.3280],\n",
      "         [-0.8812, -0.3272],\n",
      "         [-0.1327, -0.1665],\n",
      "         [ 2.0377,  1.9221],\n",
      "         [ 1.0572, -0.2264],\n",
      "         [ 2.0321, -0.1775],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.0607,  1.1104],\n",
      "         [-2.0366, -1.5471],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [-0.8951,  0.4850],\n",
      "         [-0.7530, -0.8337],\n",
      "         [-0.6991,  2.3038],\n",
      "         [ 3.3053,  1.2900],\n",
      "         [-0.8812, -0.3272],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 0.1483, -1.1064],\n",
      "         [-1.5884,  0.2484],\n",
      "         [ 2.0321, -0.1775],\n",
      "         [ 0.9355,  0.2781],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.8815, -0.5337],\n",
      "         [-0.1925,  3.2472],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [-0.9256,  0.5580]]], grad_fn=<MulBackward>)\n",
      "9\n",
      "src--> tensor([[    2,   147,    14,     4,    63,    15,   104,     4,    10, 17743,\n",
      "             7,   279,     5,     3,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,    12,   204,   129,  1103,     9,   561,   159,    13,   521,\n",
      "             8,    15,   185,     4,    53,    12,   397,  4012,    50,  2976,\n",
      "          1765,     9,    22,    13,  1493,  4539,     5,     3],\n",
      "        [    2,    12,    76,    11,    17,    67,     7,    38,   796,   976,\n",
      "             5,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,   313,    18,   112,     5,    19,   124,    23,    55,     6,\n",
      "           595,     5,     3,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,    19,    51,   135,    14,    10,   195,   241,     4,    12,\n",
      "            76,     4,    13,  3118,     4,    72,    15,    43,   259,     7,\n",
      "          9487,    82,   135,     5,     3,     1,     1,     1]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [-0.4210, -1.3046],\n",
      "         [-0.8812, -0.3272],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.3635,  2.9131],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [ 0.1458, -0.8594],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 0.7851, -1.7029],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 0.1077,  0.2387],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [ 1.1735, -2.2212],\n",
      "         [ 2.1745, -0.6827],\n",
      "         [-0.4583,  0.8505],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 1.4069,  1.0745],\n",
      "         [ 0.6511,  1.7621],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [ 1.1369,  0.0441],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-0.3550,  0.5401],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.4972,  3.2895],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-1.2385, -1.1742],\n",
      "         [ 3.4155,  0.9738],\n",
      "         [-0.5841,  0.8145],\n",
      "         [ 0.2963,  2.3704],\n",
      "         [ 1.0342, -0.5321],\n",
      "         [-0.5384,  0.7528],\n",
      "         [-0.5039,  0.3571],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [-1.0546, -0.8177],\n",
      "         [-0.2993, -0.8217],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [ 0.0888,  1.2375],\n",
      "         [-0.8815, -0.5337],\n",
      "         [ 0.1173,  1.4184],\n",
      "         [-1.9422, -0.6282],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.9192, -0.1235],\n",
      "         [ 1.3261,  0.4679],\n",
      "         [ 1.3919, -1.6461],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-1.0872, -0.7730],\n",
      "         [-1.3014,  2.2870],\n",
      "         [-1.4288, -0.0876],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.6830, -0.5819],\n",
      "         [-1.9381, -0.3054],\n",
      "         [-0.4520, -0.9377],\n",
      "         [-0.4065,  0.5112],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 2.2376, -0.2677],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-0.6830, -0.5819],\n",
      "         [-1.8924,  1.3922],\n",
      "         [ 0.6376,  2.3884],\n",
      "         [-0.8812, -0.3272],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 0.6612,  2.1578],\n",
      "         [ 2.1623, -1.0072],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [ 0.0888,  1.2375],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [-1.2882,  1.1056],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.5963,  1.6625],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [-2.3291,  0.0659],\n",
      "         [-0.7786,  1.4787],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.3305, -2.7155],\n",
      "         [-2.7675, -1.0022],\n",
      "         [ 0.6376,  2.3884],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]]], grad_fn=<MulBackward>)\n",
      "10\n",
      "src--> tensor([[    2,    12,  1901,    10,   828,    13,     6,  3909,     9,    16,\n",
      "            22,  5710,    30,  2315,     9,    44,    22,     6,   361,     8,\n",
      "             6,   330,    28,     6,  2315,     9,    75,    22,   888,     5,\n",
      "             3,     1,     1,     1,     1],\n",
      "        [    2,    12,   230,     7,    76,     4,  1540,     4,   970,     4,\n",
      "            12,    86,  4338,    69,     4,    12,   167,    28,    10,   143,\n",
      "             8,  8495,     4,    12,   112,    28,  1317,     4,    12,   186,\n",
      "            36,     6,   562,     5,     3],\n",
      "        [    2,    58,    87,  1036,  9582,     7,    38,  8419,    70, 11928,\n",
      "         22703,  1084,  2570, 16469,     4, 11317,    70, 33484, 33524,     9,\n",
      "             0,     0,     4,  2640,    70,  3175, 10047,     5,     3,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    2,    12,    22,    97,   359,     7,    38,    10,  8160,  1974,\n",
      "            28,    52,     0,  8160,     7,  3867,     5,     3,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [    2,    19,    24,    74,     9,    74,     8,    95,     4,   306,\n",
      "            93,    81,   318,     7,    29,    30,    10,   437,     8,  1783,\n",
      "           120,     4,    15,   101,   131,     4,    30,    10,   437,     8,\n",
      "          2635,     5,     3,     1,     1]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-0.2841,  0.8464],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-0.4951, -1.0024],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.2860, -1.2927],\n",
      "         [-0.5384,  0.7528],\n",
      "         [-0.0472,  0.3280],\n",
      "         [-0.5039,  0.3571],\n",
      "         [-0.1954, -3.1513],\n",
      "         [-2.6698,  0.3895],\n",
      "         [ 0.6107,  0.1781],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 2.8323, -0.9417],\n",
      "         [-0.5039,  0.3571],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.8077, -1.4990],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-0.3078,  0.3959],\n",
      "         [-3.0662,  0.0001],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.6107,  0.1781],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 0.6708,  0.8839],\n",
      "         [-0.5039,  0.3571],\n",
      "         [ 1.8053, -1.1781],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-1.8966,  2.0669],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 0.0888,  1.2375],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.5146,  1.6016],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.7017,  1.0234],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [ 0.8440,  0.5785],\n",
      "         [ 0.5006, -2.1359],\n",
      "         [-3.0459, -0.8058],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-0.9468, -0.2023],\n",
      "         [-3.0662,  0.0001],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 1.8696, -0.3991],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 2.7994, -2.4043],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-1.4288, -0.0876],\n",
      "         [-3.0662,  0.0001],\n",
      "         [-3.5061,  1.8131],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-0.6919, -2.7300],\n",
      "         [ 0.3116, -3.6889],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 1.3939,  0.0345],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 2.7113,  1.0924],\n",
      "         [ 3.1427, -0.5949],\n",
      "         [-1.7874, -0.5467],\n",
      "         [ 0.6421, -1.5527],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.9192, -0.1235],\n",
      "         [ 0.2694,  0.7321],\n",
      "         [ 0.3373, -0.0375],\n",
      "         [-0.3420, -0.6682],\n",
      "         [ 0.0150,  1.0408],\n",
      "         [-2.0096,  0.0579],\n",
      "         [-1.6002,  0.2201],\n",
      "         [-1.0786,  1.3641],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.2548, -1.4914],\n",
      "         [ 0.3373, -0.0375],\n",
      "         [ 2.2872, -0.3069],\n",
      "         [-0.9594, -0.5873],\n",
      "         [-0.5384,  0.7528],\n",
      "         [-1.3445,  4.7891],\n",
      "         [-1.3445,  4.7891],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.1216, -0.3399],\n",
      "         [ 0.3373, -0.0375],\n",
      "         [-0.6854,  0.2284],\n",
      "         [-0.2656,  0.0227],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.6375, -0.5719],\n",
      "         [-0.5039,  0.3571],\n",
      "         [ 0.3117,  1.4283],\n",
      "         [-0.0779, -1.9071],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.9192, -0.1235],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-1.5858,  1.8832],\n",
      "         [-1.2822,  0.3172],\n",
      "         [-3.0662,  0.0001],\n",
      "         [-0.9997,  1.6954],\n",
      "         [-1.3445,  4.7891],\n",
      "         [-1.5858,  1.8832],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 1.1785,  3.4341],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-0.6830, -0.5819],\n",
      "         [ 1.6521, -0.1554],\n",
      "         [ 0.2765,  3.3031],\n",
      "         [-0.5384,  0.7528],\n",
      "         [ 0.2765,  3.3031],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-0.9758, -2.8232],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 0.0282, -0.0906],\n",
      "         [-1.5682,  0.5819],\n",
      "         [-2.4933,  0.2415],\n",
      "         [-2.0595, -0.8489],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-0.4206, -1.4973],\n",
      "         [-2.6698,  0.3895],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-1.8918,  0.4571],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 1.0694,  0.7590],\n",
      "         [ 1.8614,  1.5225],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 1.2445,  0.3046],\n",
      "         [ 1.5603,  0.0474],\n",
      "         [-1.6355,  0.5754],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-2.6698,  0.3895],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-1.8918,  0.4571],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 2.7643,  0.7072],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [ 0.1719, -2.3358]]], grad_fn=<MulBackward>)\n",
      "11\n",
      "src--> tensor([[    2,   335,     5,    92,    14,    10,  3767,     8,     6,    96,\n",
      "          2082,     7,  1892,     5,     3,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    2,    92,    14,   108,    18,    43,  3727,    13,   590,     8,\n",
      "           422,  1331,  5804,     4,     9,    63,    18,   428,    28,    67,\n",
      "             6,   100,    18,    43,    67,     4,    18,    86,    10, 26344,\n",
      "            56,   272,    37,  1680,    37, 18480,  1331,  1235,    70,     6,\n",
      "           240,     8,    20,   534,     5,     3],\n",
      "        [    2, 14443,    50,    20,   330,     5,     3,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    2,  2061,    18,  5559,  3175,  1218,    98,    60,   432,     7,\n",
      "           498,    68,    50,   707,    31,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1],\n",
      "        [    2,  3838,    54,   482,     4,    44,    17,   121,     0,     6,\n",
      "         36110,     8,   124,  1813,     5,     3,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]])\n",
      "to--> tensor([[[-2.9817,  0.8635],\n",
      "         [ 1.9969,  2.6344],\n",
      "         [-0.1925,  3.2472],\n",
      "         [ 0.8445, -0.4700],\n",
      "         [-0.8812, -0.3272],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [-0.3778,  1.0985],\n",
      "         [-0.6552,  0.5439],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-1.3664,  0.5727],\n",
      "         [ 0.3153, -1.9447],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [-3.0050, -1.4665],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 0.8445, -0.4700],\n",
      "         [-0.8812, -0.3272],\n",
      "         [-0.5877, -3.0171],\n",
      "         [-1.3014,  2.2870],\n",
      "         [-2.3291,  0.0659],\n",
      "         [-1.1593, -0.3076],\n",
      "         [ 0.5091,  1.2857],\n",
      "         [ 2.9158, -0.1298],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-0.5468,  0.7313],\n",
      "         [-0.7410,  0.4196],\n",
      "         [-0.4763,  1.5816],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-0.5384,  0.7528],\n",
      "         [-0.3635,  2.9131],\n",
      "         [-1.3014,  2.2870],\n",
      "         [ 1.3729, -1.7907],\n",
      "         [-3.0662,  0.0001],\n",
      "         [-1.9422, -0.6282],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [ 0.6310, -1.5822],\n",
      "         [-1.3014,  2.2870],\n",
      "         [-2.3291,  0.0659],\n",
      "         [-1.9422, -0.6282],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [-1.3014,  2.2870],\n",
      "         [ 0.8440,  0.5785],\n",
      "         [ 0.9125, -0.5500],\n",
      "         [ 0.7348,  0.0075],\n",
      "         [-0.7530, -0.8337],\n",
      "         [-0.0872,  0.5243],\n",
      "         [-2.0366, -1.5471],\n",
      "         [-2.1332,  1.1455],\n",
      "         [-2.0366, -1.5471],\n",
      "         [-1.4048, -1.8492],\n",
      "         [-0.7410,  0.4196],\n",
      "         [-0.9603, -1.9829],\n",
      "         [ 0.3373, -0.0375],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-1.2119,  2.3594],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-0.3502, -2.2233],\n",
      "         [-0.8685,  0.3246],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-2.3028, -2.2796],\n",
      "         [-0.5841,  0.8145],\n",
      "         [-0.3502, -2.2233],\n",
      "         [-0.3078,  0.3959],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [ 1.2276,  1.9257],\n",
      "         [-1.3014,  2.2870],\n",
      "         [-1.5617,  2.9405],\n",
      "         [-0.6854,  0.2284],\n",
      "         [ 1.3082,  0.6777],\n",
      "         [ 1.2726, -1.3975],\n",
      "         [-0.4326, -1.2946],\n",
      "         [-1.5584,  2.6869],\n",
      "         [ 0.7748,  0.8265],\n",
      "         [ 2.6369,  0.8584],\n",
      "         [-1.3266, -0.8929],\n",
      "         [-0.5841,  0.8145],\n",
      "         [ 0.0512, -0.1718],\n",
      "         [ 0.8815,  0.3188],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358]],\n",
      "\n",
      "        [[-2.9817,  0.8635],\n",
      "         [-0.9181,  1.7629],\n",
      "         [-1.0169, -0.3010],\n",
      "         [-0.2137, -0.7262],\n",
      "         [ 2.2951,  1.4585],\n",
      "         [ 2.8323, -0.9417],\n",
      "         [ 0.1173,  1.4184],\n",
      "         [ 2.2512,  0.8186],\n",
      "         [-1.3445,  4.7891],\n",
      "         [ 1.2523, -0.8845],\n",
      "         [-0.4623,  0.1366],\n",
      "         [-0.6552,  0.5439],\n",
      "         [-1.9381, -0.3054],\n",
      "         [ 0.0257,  1.4509],\n",
      "         [-0.1925,  3.2472],\n",
      "         [-0.9256,  0.5580],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n",
      "         [ 0.1719, -2.3358],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [ 0.1719, -2.3358]]], grad_fn=<MulBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Embeddings(2, len(SRC.vocab))\n",
    "\n",
    "for epoch in range(1):\n",
    "    print (\"start\")\n",
    "    model.train()\n",
    "    print(\"epocstart\")\n",
    "    run_epoch(train_iter,model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
